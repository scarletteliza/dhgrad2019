{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP and entity extraction\n",
    "\n",
    "## Summary\n",
    "\n",
    "Use the SpaCy NLP library to identify named entities in a short text, then write code to parse the  output.\n",
    "\n",
    "## Details\n",
    "\n",
    "The text in question is one we've already seen (in the sutructured data exercise). To review: it's a brief pamphlet, *Growler's Income Tax* (1864), by the prolific mid-nineteenth-century writer T.S. Arthur. It's a defense of the then-new income tax, instituted in 1861 to fund the Union's war effort. As you'll see, the text is pretty straightforward, but it's kind of nifty (or infuriating, I guess) how similar are the arguments it presents concerning taxation to those you might hear today. Go ahead, download the [plain-text copy](https://github.com/wilkens-teaching/dhgrad2019/blob/master/exercises/ps4/growler.txt) and read it now, if you haven't already. It's short. (Note that you could also work with the [XML version](https://github.com/wilkens-teaching/dhgrad2019/blob/master/exercises/ps3/growler.xml) from the previous exercise and convert that to plain text as shown in the [solution to that exercise](https://github.com/wilkens-teaching/dhgrad2019/blob/master/exercises/ps3/Parsing%20structured%20data%20solution.ipynb).)\n",
    "\n",
    "Anyway, tax policy isn't really the point. Your task is to identify algorithmically the named entities in the text and to extract them for further processing. To do this, you'll use the SpaCy NLP library. SpaCy isn't included with the default Anaconda distribution, but you can install it via the Anaconda Navigator GUI or from the command line (or from a console within Jupyter lab) by typing:\n",
    "\n",
    "```\n",
    "conda install spacy\n",
    "```\n",
    "\n",
    "and then installing at least the basic trained model:\n",
    "\n",
    "```\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "SpaCy is a deep-learning-based NLP package. But the underlying details aren't super important for our purposes; we're interested in how how to use it and in how it performs on our text document. You can see the [full usage instructions](https://spacy.io/usage) at the SpaCy site; there's also some starter code below.\n",
    "\n",
    "Once you've processed the document, your task is to write code that reads the processed data and builds a list of unique entities in the output, each entity's type, and a count of how many times each entity occurs.\n",
    "\n",
    "Your program should print a summary of the entity data. Your output (with made-up data) should look roughly like this:\n",
    "\n",
    "```\n",
    "Entity\t\tType\t\tCount\n",
    "------\t\t----\t\t-----\n",
    "Boston\t\tLocation\t  2\n",
    "John Smith\tPerson\t\t1\n",
    "```\n",
    "\n",
    "## Alternative processing\n",
    "\n",
    "If you're up for a modest challenge, you might try using NLTK's (slower, lower-performing) named entity chunker. But that's strictly optional. You might also try running the SpaCy pipeline over the full set of 40 texts in the class corpus and seeing what you get.\n",
    "\n",
    "## Submit\n",
    "\n",
    "Submit your code and output as a single Jupyter notebook via Sakai.\n",
    "\n",
    "## Consider\n",
    "\n",
    "A few things to think about before class:\n",
    "\n",
    "* How well or poorly do the entities extracted from the text square with your sense of what the text is about, whom it involves, and where it occurs (or with what areas itâ€™s concerned)?\n",
    "* How accurate is the NER process?\n",
    "* How might you try to improve NER accuracy?\n",
    "\n",
    "## Starter code\n",
    "\n",
    "Here's a bit of code to get you going. Note that most of these examples involve placeholder filenames and will require changes to run on real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of a text file from disk\n",
    "with open('filename.txt', 'r') as f:\n",
    "    txt = f.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP with SpaCy\n",
    "import spacy\n",
    "import en_core_web_sm # Remember to download this model before importing!\n",
    "\n",
    "txt = \"\"\"George Washington was the first president of the United States. He was born in Virginia in 1732 and died in December of 1799.\"\"\"\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "doc = nlp(txt)\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count things\n",
    "from collections import defaultdict\n",
    "\n",
    "data = [\n",
    "    ('thing 1', 'a'), \n",
    "    ('thing 2', 'b'), \n",
    "    ('thing 3', 'b'), \n",
    "    ('thing 1', 'a'), \n",
    "    ('thing 3', 'b')\n",
    "]\n",
    "\n",
    "counts = defaultdict(dict)\n",
    "\n",
    "for item in data:\n",
    "    try:\n",
    "        counts[item[1]][item[0]] += 1\n",
    "    except:\n",
    "        counts[item[1]][item[0]] = 1\n",
    "\n",
    "# Print sorted counts\n",
    "print('tag\\tentity\\tcount')\n",
    "for tag in counts:\n",
    "    for entity in sorted(counts[tag], key=counts[tag].get, reverse=True):\n",
    "        print(f'{tag}\\t{entity}\\t{counts[tag][entity]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your code here\n",
    "\n",
    "Use the examples above to read the document from disk, process it with SpaCy, and produce a list of counted entities by type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "A few words on the quality of the output and what might help to improve it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
